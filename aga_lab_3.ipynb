{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQwOTE6YP5jt",
        "outputId": "07164934-2083-475f-9969-8eec1ef398a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.4811\n",
            "Epoch 2, Loss: 0.4811\n",
            "Epoch 3, Loss: 0.4810\n",
            "Epoch 4, Loss: 0.4810\n",
            "Epoch 5, Loss: 0.4811\n",
            "Epoch 6, Loss: 0.4811\n",
            "Epoch 7, Loss: 0.4809\n",
            "Epoch 8, Loss: 0.4810\n",
            "Epoch 9, Loss: 0.4810\n",
            "Epoch 10, Loss: 0.4810\n",
            "(60000, 128) (60000,)\n",
            "(10000, 128) (10000,)\n",
            "Classification Accuracy: 0.1339\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Constants\n",
        "VISIBLE_UNITS = 784  # 28x28 MNIST images\n",
        "HIDDEN_UNITS = 256   # Increased from 128\n",
        "BATCH_SIZE = 128     # Increased from 64 for faster training\n",
        "LEARNING_RATE = 0.01 # Reduced from 0.1 for more stable training\n",
        "MOMENTUM = 0.9       # Added momentum for faster convergence\n",
        "WEIGHT_DECAY = 1e-4  # Added regularization\n",
        "EPOCHS = 15          # Increased from 10\n",
        "CD_K = 5             # Increased CD-k steps from 1 to 5\n",
        "\n",
        "# Data loading with normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),  # MNIST mean and std\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "\n",
        "# Use try-except for data download\n",
        "try:\n",
        "    train_data = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "    test_data = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading MNIST data: {e}\")\n",
        "    raise\n",
        "\n",
        "# Data loaders with num_workers for parallel loading\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True  # Faster data transfer to GPU if available\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, visible_units, hidden_units):\n",
        "        super(RBM, self).__init__()\n",
        "        # Xavier/Glorot initialization for better convergence\n",
        "        self.W = nn.Parameter(torch.randn(hidden_units, visible_units) / np.sqrt(visible_units))\n",
        "        self.h_bias = nn.Parameter(torch.zeros(hidden_units))\n",
        "        self.v_bias = nn.Parameter(torch.zeros(visible_units))\n",
        "\n",
        "        # Track reconstruction error\n",
        "        self.reconstruction_errors = []\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        \"\"\"Calculate the free energy of a visible vector\"\"\"\n",
        "        v_bias_term = torch.matmul(v, self.v_bias)\n",
        "        hidden_term = torch.sum(torch.log(1 + torch.exp(torch.matmul(v, self.W.t()) + self.h_bias)), dim=1)\n",
        "        return -hidden_term - v_bias_term\n",
        "\n",
        "    def sample_hidden(self, v):\n",
        "        \"\"\"Sample hidden units given visible units\"\"\"\n",
        "        prob_h = torch.sigmoid(torch.matmul(v, self.W.t()) + self.h_bias)\n",
        "        h_sample = torch.bernoulli(prob_h)\n",
        "        return h_sample, prob_h\n",
        "\n",
        "    def sample_visible(self, h):\n",
        "        \"\"\"Sample visible units given hidden units\"\"\"\n",
        "        prob_v = torch.sigmoid(torch.matmul(h, self.W) + self.v_bias)\n",
        "        v_sample = torch.bernoulli(prob_v)\n",
        "        return v_sample, prob_v\n",
        "\n",
        "    def contrastive_divergence(self, v_data, k=1):\n",
        "        \"\"\"Perform k steps of contrastive divergence\"\"\"\n",
        "        # Positive phase\n",
        "        h_data, h_prob_data = self.sample_hidden(v_data)\n",
        "\n",
        "        # Negative phase (Gibbs sampling)\n",
        "        v_model = v_data.clone()\n",
        "        h_model = h_data.clone()\n",
        "\n",
        "        for _ in range(k):\n",
        "            v_model, v_prob_model = self.sample_visible(h_model)\n",
        "            h_model, h_prob_model = self.sample_hidden(v_model)\n",
        "\n",
        "        # Calculate gradients\n",
        "        W_grad = torch.matmul(h_prob_data.t(), v_data) - torch.matmul(h_prob_model.t(), v_prob_model)\n",
        "        h_bias_grad = torch.sum(h_prob_data - h_prob_model, dim=0)\n",
        "        v_bias_grad = torch.sum(v_data - v_prob_model, dim=0)\n",
        "\n",
        "        return v_prob_model, W_grad, h_bias_grad, v_bias_grad\n",
        "\n",
        "    def forward(self, v):\n",
        "        \"\"\"Forward pass for feature extraction\"\"\"\n",
        "        _, h_prob = self.sample_hidden(v)\n",
        "        return h_prob\n",
        "\n",
        "    def reconstruct(self, v):\n",
        "        \"\"\"Reconstruct visible units\"\"\"\n",
        "        h_sample, _ = self.sample_hidden(v)\n",
        "        _, v_prob = self.sample_visible(h_sample)\n",
        "        return v_prob\n",
        "\n",
        "def train_rbm(rbm, optimizer, epochs=EPOCHS):\n",
        "    \"\"\"Train the RBM model\"\"\"\n",
        "    start_time = time.time()\n",
        "    rbm.train()\n",
        "\n",
        "    # Lists to track metrics\n",
        "    epoch_losses = []\n",
        "    reconstruction_errors = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        epoch_recon_error = 0.0\n",
        "\n",
        "        # Use tqdm for progress bar\n",
        "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
        "            for data, _ in pbar:\n",
        "                data = data.to(device)\n",
        "\n",
        "                # Perform CD-k\n",
        "                v_recon, W_grad, h_bias_grad, v_bias_grad = rbm.contrastive_divergence(data, k=CD_K)\n",
        "\n",
        "                # Compute reconstruction error\n",
        "                recon_error = torch.mean(torch.sum((data - v_recon)**2, dim=1))\n",
        "\n",
        "                # Manual parameter update (alternative to backward)\n",
        "                with torch.no_grad():\n",
        "                    rbm.W.add_(LEARNING_RATE * W_grad / len(data))\n",
        "                    rbm.h_bias.add_(LEARNING_RATE * h_bias_grad / len(data))\n",
        "                    rbm.v_bias.add_(LEARNING_RATE * v_bias_grad / len(data))\n",
        "\n",
        "                epoch_loss += recon_error.item()\n",
        "                epoch_recon_error += recon_error.item()\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix({\"loss\": f\"{recon_error.item():.4f}\"})\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        avg_recon_error = epoch_recon_error / len(train_loader)\n",
        "        epoch_losses.append(avg_loss)\n",
        "        reconstruction_errors.append(avg_recon_error)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Recon. Error: {avg_recon_error:.4f}\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"RBM training completed in {total_time:.2f} seconds\")\n",
        "\n",
        "    return epoch_losses, reconstruction_errors\n",
        "\n",
        "def visualize_weights(rbm, num_weights=10):\n",
        "    \"\"\"Visualize RBM weights for selected hidden units\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i in range(num_weights):\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        weight = rbm.W[i].detach().cpu().numpy().reshape(28, 28)\n",
        "        plt.imshow(weight, cmap='viridis')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Hidden Unit {i+1}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rbm_weights.png')\n",
        "    plt.close()\n",
        "\n",
        "def visualize_reconstruction(rbm, test_loader):\n",
        "    \"\"\"Visualize original and reconstructed images\"\"\"\n",
        "    rbm.eval()\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Get a batch of test data\n",
        "    data, _ = next(iter(test_loader))\n",
        "    data = data.to(device)\n",
        "\n",
        "    # Reconstruct the data\n",
        "    with torch.no_grad():\n",
        "        recon = rbm.reconstruct(data)\n",
        "\n",
        "    # Plot original and reconstructed images\n",
        "    for i in range(5):\n",
        "        # Original\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        plt.imshow(data[i].cpu().numpy().reshape(28, 28), cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Original {i+1}')\n",
        "\n",
        "        # Reconstructed\n",
        "        plt.subplot(2, 5, i+6)\n",
        "        plt.imshow(recon[i].cpu().numpy().reshape(28, 28), cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Recon {i+1}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rbm_reconstruction.png')\n",
        "    plt.close()\n",
        "\n",
        "def extract_features(rbm, data_loader):\n",
        "    \"\"\"Extract features using the trained RBM\"\"\"\n",
        "    features, labels = [], []\n",
        "    rbm.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(data_loader, desc=\"Extracting features\"):\n",
        "            data = data.to(device)\n",
        "            h_prob = rbm(data)\n",
        "            features.append(h_prob.cpu().numpy())\n",
        "            labels.append(target.numpy())\n",
        "\n",
        "    return np.vstack(features), np.hstack(labels)\n",
        "\n",
        "# Initialize model and move to device\n",
        "rbm = RBM(visible_units=VISIBLE_UNITS, hidden_units=HIDDEN_UNITS).to(device)\n",
        "\n",
        "# Use SGD with momentum and weight decay\n",
        "optimizer = optim.SGD(\n",
        "    rbm.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    momentum=MOMENTUM,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# Train RBM\n",
        "losses, recon_errors = train_rbm(rbm, optimizer)\n",
        "\n",
        "# Visualize weights and reconstructions\n",
        "visualize_weights(rbm)\n",
        "visualize_reconstruction(rbm, test_loader)\n",
        "\n",
        "# Plot training metrics\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.title('RBM Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Reconstruction Error')\n",
        "plt.grid(True)\n",
        "plt.savefig('rbm_training_loss.png')\n",
        "plt.close()\n",
        "\n",
        "# Extract features\n",
        "print(\"Extracting features for training set...\")\n",
        "X_train, y_train = extract_features(rbm, train_loader)\n",
        "print(\"Extracting features for test set...\")\n",
        "X_test, y_test = extract_features(rbm, test_loader)\n",
        "\n",
        "print(f\"Train features shape: {X_train.shape}, Train labels shape: {y_train.shape}\")\n",
        "print(f\"Test features shape: {X_test.shape}, Test labels shape: {y_test.shape}\")\n",
        "\n",
        "# Train different classifiers and compare\n",
        "print(\"\\nTraining classifiers on RBM features:\")\n",
        "\n",
        "# 1. Logistic Regression\n",
        "print(\"\\n1. Logistic Regression\")\n",
        "start_time = time.time()\n",
        "lr_clf = LogisticRegression(max_iter=2000, C=1.0, solver='lbfgs', multi_class='multinomial', n_jobs=-1)\n",
        "lr_clf.fit(X_train, y_train)\n",
        "lr_pred = lr_clf.predict(X_test)\n",
        "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
        "print(f\"Training time: {time.time() - start_time:.2f} seconds\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, lr_pred))\n",
        "\n",
        "# 2. SVM (optional - can be computationally expensive)\n",
        "from sklearn.svm import SVC\n",
        "print(\"\\n2. SVM Classifier\")\n",
        "start_time = time.time()\n",
        "svm_clf = SVC(kernel='rbf', C=10, gamma='scale')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "svm_pred = svm_clf.predict(X_test)\n",
        "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
        "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
        "print(f\"Training time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# 3. Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "print(\"\\n3. Random Forest Classifier\")\n",
        "start_time = time.time()\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=20, n_jobs=-1)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"Training time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Compare models\n",
        "print(\"\\nModel Comparison:\")\n",
        "models = {\n",
        "    'Logistic Regression': lr_accuracy,\n",
        "    'SVM': svm_accuracy,\n",
        "    'Random Forest': rf_accuracy\n",
        "}\n",
        "\n",
        "for model, acc in sorted(models.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{model}: {acc:.4f}\")\n",
        "\n",
        "# Experiment with different hidden unit counts (optional)\n",
        "def experiment_hidden_units():\n",
        "    \"\"\"Experiment with different numbers of hidden units\"\"\"\n",
        "    hidden_units_list = [64, 128, 256, 512]\n",
        "    accuracies = []\n",
        "\n",
        "    for h_units in hidden_units_list:\n",
        "        print(f\"\\nTraining RBM with {h_units} hidden units\")\n",
        "\n",
        "        # Create and train RBM\n",
        "        test_rbm = RBM(visible_units=VISIBLE_UNITS, hidden_units=h_units).to(device)\n",
        "        train_rbm(test_rbm, optim.SGD(test_rbm.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM), epochs=5)\n",
        "\n",
        "        # Extract features and train classifier\n",
        "        X_train_exp, y_train_exp = extract_features(test_rbm, train_loader)\n",
        "        X_test_exp, y_test_exp = extract_features(test_rbm, test_loader)\n",
        "\n",
        "        lr_clf = LogisticRegression(max_iter=1000)\n",
        "        lr_clf.fit(X_train_exp, y_train_exp)\n",
        "        y_pred_exp = lr_clf.predict(X_test_exp)\n",
        "\n",
        "        accuracy = accuracy_score(y_test_exp, y_pred_exp)\n",
        "        accuracies.append(accuracy)\n",
        "        print(f\"Classification accuracy with {h_units} hidden units: {accuracy:.4f}\")\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(hidden_units_list, accuracies, 'o-')\n",
        "    plt.title('Classification Accuracy vs. Number of Hidden Units')\n",
        "    plt.xlabel('Number of Hidden Units')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('hidden_units_experiment.png')\n",
        "    plt.close()\n",
        "\n",
        "# Uncomment to run the experiment (takes time)\n",
        "# experiment_hidden_units()\n",
        "\n",
        "print(\"\\nExecution complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, visible_units, hidden_units):\n",
        "        super(RBM, self).__init__()\n",
        "        self.W = nn.Parameter(torch.randn(hidden_units, visible_units) * 0.01)\n",
        "        self.h_bias = nn.Parameter(torch.zeros(hidden_units))\n",
        "        self.v_bias = nn.Parameter(torch.zeros(visible_units))\n",
        "\n",
        "    def sample_hidden(self, v):\n",
        "        prob_h = torch.sigmoid(torch.matmul(v, self.W.t()) + self.h_bias)\n",
        "        return torch.bernoulli(prob_h), prob_h\n",
        "\n",
        "    def sample_visible(self, h):\n",
        "        prob_v = torch.sigmoid(torch.matmul(h, self.W) + self.v_bias)\n",
        "        return torch.bernoulli(prob_v), prob_v\n",
        "\n",
        "    def contrastive_divergence(self, v, k=1):\n",
        "        v_sample = v\n",
        "        for _ in range(k):\n",
        "            h_sample, _ = self.sample_hidden(v_sample)\n",
        "            v_sample, _ = self.sample_visible(h_sample)\n",
        "        return v_sample\n",
        "\n",
        "class StackRBM:\n",
        "    def __init__(self, size=5):\n",
        "        self.stack_size = size\n",
        "        self.stack = torch.zeros((1, size))\n",
        "        self.rbm = RBM(visible_units=size, hidden_units=size)\n",
        "        self.optimizer = optim.SGD(self.rbm.parameters(), lr=0.1)\n",
        "\n",
        "    def push(self, value):\n",
        "        self.stack = torch.roll(self.stack, shifts=1, dims=1)\n",
        "        self.stack[0, 0] = value\n",
        "        self.train_rbm()\n",
        "\n",
        "    def pop(self):\n",
        "        top_value = self.stack[0, 0].item()\n",
        "        self.stack[0, 0] = 0\n",
        "        self.stack = torch.roll(self.stack, shifts=-1, dims=1)\n",
        "        self.train_rbm()\n",
        "        return top_value\n",
        "\n",
        "    def train_rbm(self):\n",
        "        v_sample = self.rbm.contrastive_divergence(self.stack)\n",
        "        loss = torch.mean((self.stack - v_sample) ** 2)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def display_stack(self):\n",
        "        print(\"Stack State:\", self.stack.numpy())\n",
        "\n",
        "stack_rbm = StackRBM(size=5)\n",
        "stack_rbm.push(1)\n",
        "stack_rbm.push(2)\n",
        "stack_rbm.push(3)\n",
        "stack_rbm.display_stack()\n",
        "print(\"Popped:\", stack_rbm.pop())\n",
        "stack_rbm.display_stack()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EZzcATgWIvN",
        "outputId": "c8d40458-3392-4e24-a061-6497f5a8072a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stack State: [[3. 2. 1. 0. 0.]]\n",
            "Popped: 3.0\n",
            "Stack State: [[2. 1. 0. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}